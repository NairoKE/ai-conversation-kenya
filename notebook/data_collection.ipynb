{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# AI in Kenya - Data Collection\n",
        "\n",
        "This notebook handles the collection of data from Twitter and LinkedIn about AI conversations in Kenya.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure rate limit handler\n",
        "def handle_rate_limit(cursor):\n",
        "    while True:\n",
        "        try:\n",
        "            yield next(cursor)\n",
        "        except tweepy.RateLimitError:\n",
        "            logger.info(\"Rate limit reached. Waiting for 15 minutes...\")\n",
        "            time.sleep(15 * 60)  # Wait for 15 minutes\n",
        "        except StopIteration:\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error: {str(e)}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Twitter Data Collection\n",
        "\n",
        "We'll use Tweepy to collect tweets about AI in Kenya, including user metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Twitter API Authentication\n",
        "try:\n",
        "    client = tweepy.Client(\n",
        "        bearer_token=os.getenv('TWITTER_BEARER_TOKEN'),\n",
        "        consumer_key=os.getenv('TWITTER_API_KEY'),\n",
        "        consumer_secret=os.getenv('TWITTER_API_SECRET'),\n",
        "        access_token=os.getenv('TWITTER_ACCESS_TOKEN'),\n",
        "        access_token_secret=os.getenv('TWITTER_ACCESS_TOKEN_SECRET'),\n",
        "        wait_on_rate_limit=True\n",
        "    )\n",
        "    logger.info(\"Successfully authenticated with Twitter API\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error authenticating with Twitter API: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized search queries with targeted keywords and operators\n",
        "search_queries = [\n",
        "    '(AI OR \"artificial intelligence\") (Kenya OR Nairobi) -is:retweet',\n",
        "    '\"digital transformation\" (Kenya OR Nairobi) -is:retweet',\n",
        "    '\"machine learning\" (Kenya OR Nairobi) -is:retweet',\n",
        "    '(tech OR technology) (upskilling OR reskilling) (Kenya OR Nairobi) -is:retweet',\n",
        "    'AI (startup OR innovation) (Kenya OR Nairobi) -is:retweet'\n",
        "]\n",
        "\n",
        "# Define key organizations to track\n",
        "key_organizations = [\n",
        "    'Safaricom',\n",
        "    'KCBGroup',\n",
        "    'EquityBank',\n",
        "    'iHub',\n",
        "    'MoringaSchool',\n",
        "    'Microsoft_EA',\n",
        "    'Google_Kenya',\n",
        "    'IBMEastAfrica'\n",
        "]\n",
        "\n",
        "def collect_tweets_with_metadata(query, max_results=100):\n",
        "    \"\"\"\n",
        "    Collect tweets with optimized metadata collection and error handling\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    tweet_fields = ['created_at', 'public_metrics', 'context_annotations', 'entities']\n",
        "    user_fields = ['username', 'public_metrics', 'verified', 'description']\n",
        "    \n",
        "    try:\n",
        "        # Search tweets with pagination\n",
        "        pagination_token = None\n",
        "        while len(tweets) < max_results:\n",
        "            response = client.search_recent_tweets(\n",
        "                query=query,\n",
        "                max_results=min(100, max_results - len(tweets)),  # Adjust batch size based on remaining needed\n",
        "                tweet_fields=tweet_fields,\n",
        "                user_fields=user_fields,\n",
        "                expansions=['author_id'],\n",
        "                next_token=pagination_token\n",
        "            )\n",
        "            \n",
        "            if not response.data:\n",
        "                break\n",
        "                \n",
        "            # Process users lookup\n",
        "            users = {user.id: user for user in response.includes['users']} if response.includes else {}\n",
        "            \n",
        "            for tweet in response.data:\n",
        "                user = users.get(tweet.author_id, {})\n",
        "                \n",
        "                tweet_data = {\n",
        "                    'created_at': tweet.created_at,\n",
        "                    'text': tweet.text,\n",
        "                    'username': user.username if user else None,\n",
        "                    'user_followers': user.public_metrics['followers_count'] if user else None,\n",
        "                    'user_verified': user.verified if user else None,\n",
        "                    'retweet_count': tweet.public_metrics['retweet_count'],\n",
        "                    'like_count': tweet.public_metrics['like_count'],\n",
        "                    'reply_count': tweet.public_metrics['reply_count'],\n",
        "                    'quote_count': tweet.public_metrics['quote_count'],\n",
        "                    'query': query\n",
        "                }\n",
        "                tweets.append(tweet_data)\n",
        "            \n",
        "            if not response.meta.get('next_token'):\n",
        "                break\n",
        "            \n",
        "            pagination_token = response.meta['next_token']\n",
        "            logger.info(f\"Collected {len(tweets)} tweets for query: {query}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error collecting tweets for query {query}: {str(e)}\")\n",
        "    \n",
        "    return tweets\n",
        "\n",
        "def collect_organization_tweets(org_handle, max_results=50):\n",
        "    \"\"\"\n",
        "    Collect tweets from specific organizations\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get user ID first\n",
        "        user = client.get_user(username=org_handle)\n",
        "        if not user.data:\n",
        "            logger.warning(f\"Could not find user {org_handle}\")\n",
        "            return []\n",
        "            \n",
        "        user_id = user.data.id\n",
        "        tweets = []\n",
        "        \n",
        "        # Get tweets from the organization\n",
        "        response = client.get_users_tweets(\n",
        "            user_id,\n",
        "            max_results=max_results,\n",
        "            tweet_fields=['created_at', 'public_metrics'],\n",
        "            exclude=['retweets', 'replies']\n",
        "        )\n",
        "        \n",
        "        if not response.data:\n",
        "            return []\n",
        "            \n",
        "        for tweet in response.data:\n",
        "            if any(ai_term.lower() in tweet.text.lower() for ai_term in ['ai', 'artificial intelligence', 'machine learning', 'digital']):\n",
        "                tweet_data = {\n",
        "                    'created_at': tweet.created_at,\n",
        "                    'text': tweet.text,\n",
        "                    'username': org_handle,\n",
        "                    'retweet_count': tweet.public_metrics['retweet_count'],\n",
        "                    'like_count': tweet.public_metrics['like_count'],\n",
        "                    'reply_count': tweet.public_metrics['reply_count'],\n",
        "                    'quote_count': tweet.public_metrics['quote_count'],\n",
        "                    'source': 'organization_timeline'\n",
        "                }\n",
        "                tweets.append(tweet_data)\n",
        "                \n",
        "        logger.info(f\"Collected {len(tweets)} relevant tweets from {org_handle}\")\n",
        "        return tweets\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error collecting tweets from {org_handle}: {str(e)}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main data collection function with optimized API usage\n",
        "def collect_all_data(max_tweets_per_query=50, max_tweets_per_org=30):\n",
        "    \"\"\"\n",
        "    Collect data from both search queries and organization timelines\n",
        "    with optimized API usage\n",
        "    \"\"\"\n",
        "    all_tweets = []\n",
        "    \n",
        "    # First collect from organizations (more targeted data)\n",
        "    logger.info(\"Collecting tweets from key organizations...\")\n",
        "    for org in key_organizations:\n",
        "        org_tweets = collect_organization_tweets(org, max_results=max_tweets_per_org)\n",
        "        all_tweets.extend(org_tweets)\n",
        "        logger.info(f\"Collected {len(org_tweets)} tweets from {org}\")\n",
        "        \n",
        "    # Then collect from search queries\n",
        "    logger.info(\"Collecting tweets from search queries...\")\n",
        "    for query in search_queries:\n",
        "        query_tweets = collect_tweets_with_metadata(query, max_results=max_tweets_per_query)\n",
        "        all_tweets.extend(query_tweets)\n",
        "        logger.info(f\"Collected {len(query_tweets)} tweets for query: {query}\")\n",
        "    \n",
        "    # Convert to DataFrame and remove duplicates\n",
        "    df = pd.DataFrame(all_tweets)\n",
        "    df = df.drop_duplicates(subset=['text'])\n",
        "    \n",
        "    # Save to CSV with timestamp\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f'../data/twitter_data_{timestamp}.csv'\n",
        "    df.to_csv(filename, index=False)\n",
        "    logger.info(f\"Saved {len(df)} unique tweets to {filename}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Execute data collection\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df = collect_all_data()\n",
        "        print(f\"Successfully collected {len(df)} tweets\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main data collection: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define search keywords and tweet collection function\n",
        "search_queries = [\n",
        "    'AI Kenya',\n",
        "    'digital transformation Kenya',\n",
        "    'machine learning Kenya',\n",
        "    'future-proof workforce Kenya',\n",
        "    'AI reskilling Kenya'\n",
        "]\n",
        "\n",
        "def collect_tweets(query, max_tweets=100):\n",
        "    tweets = []\n",
        "    try:\n",
        "        for tweet in tweepy.Cursor(api.search_tweets,\n",
        "                                  q=query,\n",
        "                                  lang='en',\n",
        "                                  tweet_mode='extended').items(max_tweets):\n",
        "            tweet_data = {\n",
        "                'created_at': tweet.created_at,\n",
        "                'text': tweet.full_text,\n",
        "                'username': tweet.user.screen_name,\n",
        "                'user_followers': tweet.user.followers_count,\n",
        "                'user_verified': tweet.user.verified,\n",
        "                'retweet_count': tweet.retweet_count,\n",
        "                'favorite_count': tweet.favorite_count,\n",
        "                'query': query\n",
        "            }\n",
        "            tweets.append(tweet_data)\n",
        "    except Exception as e:\n",
        "        print(f'Error collecting tweets for query {query}: {str(e)}')\n",
        "    \n",
        "    return tweets\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## LinkedIn Data Template\n",
        "\n",
        "Structure for manual LinkedIn data collection with focus on seniority levels and company sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LinkedIn data collection template\n",
        "linkedin_columns = [\n",
        "    'post_date',\n",
        "    'author_name',\n",
        "    'author_title',\n",
        "    'author_company',\n",
        "    'seniority_level',  # C-level, Director, Manager, Individual Contributor\n",
        "    'company_size',     # Small (<50), Medium (50-500), Large (>500)\n",
        "    'post_text',\n",
        "    'likes',\n",
        "    'comments',\n",
        "    'shares'\n",
        "]\n",
        "\n",
        "linkedin_df = pd.DataFrame(columns=linkedin_columns)\n",
        "linkedin_df.to_csv('../data/linkedin_template.csv', index=False)\n",
        "print(\"LinkedIn template created at '../data/linkedin_template.csv'\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Processing and Storage\n",
        "\n",
        "Functions to process and store collected data from both platforms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process and store Twitter data\n",
        "def process_twitter_data():\n",
        "    all_tweets = []\n",
        "    for query in search_queries:\n",
        "        tweets = collect_tweets(query)\n",
        "        all_tweets.extend(tweets)\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_tweets)\n",
        "    \n",
        "    # Save to CSV\n",
        "    df.to_csv('../data/twitter_data.csv', index=False)\n",
        "    print(f'Saved {len(df)} tweets to data/twitter_data.csv')\n",
        "    \n",
        "    return df\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
